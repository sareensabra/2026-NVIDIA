{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06088c92-a1be-4571-9d00-0fe691c43b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudaq\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from typing import List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import deque\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================\n",
    "# GPU ACCELERATION: Import CuPy (falls back to NumPy if unavailable)\n",
    "# ============================================================\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU_AVAILABLE = True\n",
    "    print(\"[GPU] CuPy detected - GPU acceleration enabled\")\n",
    "except ImportError:\n",
    "    import numpy as cp  # Fallback to NumPy\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"[CPU] CuPy not found - using CPU (install with: pip install cupy-cuda12x)\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# GPU-OPTIMIZED ENERGY COMPUTATION\n",
    "# ============================================================\n",
    "def compute_energy_fft_gpu(sequence: np.ndarray) -> float:\n",
    "    \"\"\"GPU-accelerated FFT energy computation.\"\"\"\n",
    "    n = len(sequence)\n",
    "    \n",
    "    if GPU_AVAILABLE:\n",
    "        # Transfer to GPU\n",
    "        seq_gpu = cp.asarray(sequence, dtype=cp.float32)\n",
    "        padded = cp.zeros(2 * n, dtype=cp.float32)\n",
    "        padded[:n] = seq_gpu\n",
    "        \n",
    "        # FFT on GPU\n",
    "        fft_result = cp.fft.fft(padded)\n",
    "        power_spectrum = cp.abs(fft_result) ** 2\n",
    "        autocorr = cp.fft.ifft(power_spectrum).real\n",
    "        autocorr = autocorr[:n]\n",
    "        energy = cp.sum(autocorr[1:] ** 2)\n",
    "        \n",
    "        # Transfer back to CPU\n",
    "        return float(energy.get())\n",
    "    else:\n",
    "        # CPU fallback\n",
    "        padded = np.zeros(2 * n)\n",
    "        padded[:n] = sequence\n",
    "        fft_result = np.fft.fft(padded)\n",
    "        power_spectrum = np.abs(fft_result) ** 2\n",
    "        autocorr = np.fft.ifft(power_spectrum).real[:n]\n",
    "        return float(np.sum(autocorr[1:] ** 2))\n",
    "\n",
    "\n",
    "def compute_energy_batch_gpu(population: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    GPU OPTIMIZATION: Batch evaluate entire population in parallel.\n",
    "    This is much faster than individual evaluations.\n",
    "    \"\"\"\n",
    "    n_sequences, n_qubits = population.shape\n",
    "    \n",
    "    if GPU_AVAILABLE:\n",
    "        # Transfer entire population to GPU at once\n",
    "        pop_gpu = cp.asarray(population, dtype=cp.float32)\n",
    "        padded = cp.zeros((n_sequences, 2 * n_qubits), dtype=cp.float32)\n",
    "        padded[:, :n_qubits] = pop_gpu\n",
    "        \n",
    "        # Batch FFT (processes all sequences in parallel!)\n",
    "        fft_results = cp.fft.fft(padded, axis=1)\n",
    "        power_spectra = cp.abs(fft_results) ** 2\n",
    "        autocorrs = cp.fft.ifft(power_spectra, axis=1).real[:, :n_qubits]\n",
    "        \n",
    "        # Compute energies for all sequences\n",
    "        energies = cp.sum(autocorrs[:, 1:] ** 2, axis=1)\n",
    "        \n",
    "        return energies.get()  # Transfer back to CPU\n",
    "    else:\n",
    "        # CPU fallback\n",
    "        return np.array([compute_energy_fft_gpu(seq) for seq in population])\n",
    "\n",
    "\n",
    "def compute_merit_factor_fft(sequence: np.ndarray) -> float:\n",
    "    \"\"\"Compute merit factor from energy.\"\"\"\n",
    "    n = len(sequence)\n",
    "    energy = compute_energy_fft_gpu(sequence)\n",
    "    if energy == 0:\n",
    "        return float('inf')\n",
    "    return n ** 2 / (2 * energy)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CUDA-Q QUANTUM COMPONENTS (Already GPU-accelerated)\n",
    "# ============================================================\n",
    "def build_labs_hamiltonian(n_qubits: int) -> cudaq.SpinOperator:\n",
    "    \"\"\"Build LABS Hamiltonian for VQE.\"\"\"\n",
    "    hamiltonian = cudaq.SpinOperator()\n",
    "    \n",
    "    for k in range(1, n_qubits):\n",
    "        for i in range(n_qubits - k):\n",
    "            for j in range(n_qubits - k):\n",
    "                z_count = [0] * n_qubits\n",
    "                for idx in [i, i + k, j, j + k]:\n",
    "                    z_count[idx] += 1\n",
    "                \n",
    "                term = cudaq.SpinOperator()\n",
    "                has_z = False\n",
    "                \n",
    "                for site in range(n_qubits):\n",
    "                    if z_count[site] % 2 == 1:\n",
    "                        if not has_z:\n",
    "                            term = cudaq.spin.z(site)\n",
    "                            has_z = True\n",
    "                        else:\n",
    "                            term *= cudaq.spin.z(site)\n",
    "                \n",
    "                if has_z:\n",
    "                    hamiltonian += term\n",
    "                else:\n",
    "                    hamiltonian += cudaq.SpinOperator()\n",
    "    \n",
    "    return hamiltonian\n",
    "\n",
    "\n",
    "# Global variables for kernel (CUDA-Q requirement)\n",
    "n_qubits_global = 7\n",
    "n_layers_global = 5\n",
    "\n",
    "\n",
    "def set_circuit_params(n_qubits: int, n_layers: int):\n",
    "    \"\"\"Set global circuit parameters.\"\"\"\n",
    "    global n_qubits_global, n_layers_global\n",
    "    n_qubits_global = n_qubits\n",
    "    n_layers_global = n_layers\n",
    "\n",
    "\n",
    "@cudaq.kernel\n",
    "def hea_ansatz(qubits: cudaq.qview, params: List[float], n_layers: int):\n",
    "    \"\"\"Hardware-efficient ansatz.\"\"\"\n",
    "    n_qubits = qubits.size()\n",
    "    param_idx = 0\n",
    "    \n",
    "    for layer in range(n_layers):\n",
    "        for qubit in range(n_qubits):\n",
    "            ry(params[param_idx], qubits[qubit])\n",
    "            param_idx += 1\n",
    "            rz(params[param_idx], qubits[qubit])\n",
    "            param_idx += 1\n",
    "        \n",
    "        for qubit in range(n_qubits - 1):\n",
    "            x.ctrl(qubits[qubit], qubits[qubit + 1])\n",
    "        \n",
    "        x.ctrl(qubits[n_qubits - 1], qubits[0])\n",
    "\n",
    "\n",
    "@cudaq.kernel\n",
    "def vqe_circuit(params: List[float]):\n",
    "    qubits = cudaq.qvector(n_qubits_global)\n",
    "    hea_ansatz(qubits, params, n_layers_global)\n",
    "\n",
    "\n",
    "@cudaq.kernel\n",
    "def vqe_circuit_measure(params: List[float]):\n",
    "    qubits = cudaq.qvector(n_qubits_global)\n",
    "    hea_ansatz(qubits, params, n_layers_global)\n",
    "    mz(qubits)\n",
    "\n",
    "\n",
    "def run_vqe_optimization(\n",
    "    hamiltonian: cudaq.SpinOperator,\n",
    "    initial_params: np.ndarray,\n",
    "    maxiter: int = 200\n",
    ") -> Tuple[np.ndarray, float, List[float]]:\n",
    "    \"\"\"\n",
    "    VQE optimization - already GPU-accelerated via CUDA-Q.\n",
    "    cudaq.observe() automatically uses GPU when available.\n",
    "    \"\"\"\n",
    "    energy_history = []\n",
    "    \n",
    "    def objective(params):\n",
    "        result = cudaq.observe(vqe_circuit, hamiltonian, params.tolist())\n",
    "        energy = result.expectation()\n",
    "        energy_history.append(energy)\n",
    "        return energy\n",
    "    \n",
    "    result = minimize(\n",
    "        objective,\n",
    "        initial_params,\n",
    "        method='COBYLA',\n",
    "        options={'maxiter': maxiter, 'rhobeg': 0.5, 'tol': 1e-6}\n",
    "    )\n",
    "    \n",
    "    return result.x, result.fun, energy_history\n",
    "\n",
    "\n",
    "def sample_population(optimized_params: np.ndarray, n_samples: int = 100, seed: int = 42):\n",
    "    \"\"\"Sample from optimized VQE state.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    counts = cudaq.sample(vqe_circuit_measure, optimized_params.tolist(), \n",
    "                          shots_count=n_samples)\n",
    "    return counts\n",
    "\n",
    "\n",
    "def convert_sample_to_arr(sample, N, shots=100): \n",
    "    \"\"\"Convert CUDA-Q sample to numpy array.\"\"\"\n",
    "    arr = np.zeros((shots, N), dtype=int)\n",
    "    idx = 0\n",
    "    for bitstring, count in sample.items():\n",
    "        for _ in range(count):\n",
    "            row = np.array([int(b) for b in bitstring], dtype=int)\n",
    "            row[row == 0] = -1\n",
    "            arr[idx, :] = row\n",
    "            idx += 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "def generate_quantum_pop(n_qubits: int, n_layers: int, n_pop: int, max_iter: int = 200):\n",
    "    \"\"\"Generate quantum population via VQE.\"\"\"\n",
    "    set_circuit_params(n_qubits, n_layers)\n",
    "    n_params = 2 * n_qubits * n_layers\n",
    "\n",
    "    print(\"\\n[1] Building LABS Hamiltonian...\")\n",
    "    hamiltonian = build_labs_hamiltonian(n_qubits)\n",
    "\n",
    "    print(\"\\n[2] Initializing variational parameters...\")\n",
    "    np.random.seed(42)\n",
    "    initial_params = np.random.uniform(-np.pi/4, np.pi/4, n_params)\n",
    "\n",
    "    print(\"\\n[3] Running VQE optimization (GPU-accelerated via CUDA-Q)...\")\n",
    "    optimized_params, final_energy, history = run_vqe_optimization(\n",
    "        hamiltonian, initial_params, maxiter=max_iter\n",
    "    )\n",
    "\n",
    "    print(f\"    Final energy: {final_energy:.6f}\")\n",
    "\n",
    "    print(\"\\n[4] Sampling population for classical seeding...\")\n",
    "    counts = sample_population(optimized_params, n_samples=n_pop)\n",
    "\n",
    "    quantum_pop = convert_sample_to_arr(counts, n_qubits, n_pop)\n",
    "\n",
    "    print(f\"\\n[5] Population generated, Size: {np.shape(quantum_pop)}\")\n",
    "    return history, quantum_pop\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# GPU-OPTIMIZED TABU SEARCH\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class TabuSearchConfig:\n",
    "    tabu_tenure: int = 7\n",
    "    max_iterations: int = 1000\n",
    "    aspiration_threshold: float = 0.0\n",
    "\n",
    "\n",
    "class TabuList:\n",
    "    def __init__(self, tenure: int):\n",
    "        self.tenure = tenure\n",
    "        self.tabu_moves = deque(maxlen=tenure)\n",
    "        self.tabu_set = set()\n",
    "    \n",
    "    def add(self, move: int):\n",
    "        if len(self.tabu_moves) == self.tenure:\n",
    "            old_move = self.tabu_moves[0]\n",
    "            self.tabu_set.discard(old_move)\n",
    "        self.tabu_moves.append(move)\n",
    "        self.tabu_set.add(move)\n",
    "    \n",
    "    def is_tabu(self, move: int) -> bool:\n",
    "        return move in self.tabu_set\n",
    "    \n",
    "    def clear(self):\n",
    "        self.tabu_moves.clear()\n",
    "        self.tabu_set.clear()\n",
    "\n",
    "\n",
    "def evaluate_all_flips_gpu(sequence: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    GPU OPTIMIZATION: Evaluate all possible single-bit flips in parallel.\n",
    "    Instead of flipping and computing one at a time, do all at once.\n",
    "    \"\"\"\n",
    "    n = len(sequence)\n",
    "    \n",
    "    if GPU_AVAILABLE:\n",
    "        # Create all flipped variants at once\n",
    "        seq_gpu = cp.asarray(sequence, dtype=cp.float32)\n",
    "        # Tile sequence n times\n",
    "        all_variants = cp.tile(seq_gpu, (n, 1))\n",
    "        # Flip diagonal (each row flips a different position)\n",
    "        flip_mask = cp.eye(n, dtype=cp.float32) * -2  # -2 because we want to flip: x -> -x means multiply by -1, or add -2x\n",
    "        all_variants = all_variants + flip_mask * cp.tile(seq_gpu, (n, 1))\n",
    "        \n",
    "        # Alternative: direct flip\n",
    "        all_variants = cp.tile(seq_gpu, (n, 1))\n",
    "        for i in range(n):\n",
    "            all_variants[i, i] *= -1\n",
    "        \n",
    "        # Batch compute energies\n",
    "        padded = cp.zeros((n, 2 * n), dtype=cp.float32)\n",
    "        padded[:, :n] = all_variants\n",
    "        \n",
    "        fft_results = cp.fft.fft(padded, axis=1)\n",
    "        power_spectra = cp.abs(fft_results) ** 2\n",
    "        autocorrs = cp.fft.ifft(power_spectra, axis=1).real[:, :n]\n",
    "        energies = cp.sum(autocorrs[:, 1:] ** 2, axis=1)\n",
    "        \n",
    "        return energies.get()\n",
    "    else:\n",
    "        # CPU fallback\n",
    "        energies = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            test_seq = sequence.copy()\n",
    "            test_seq[i] *= -1\n",
    "            energies[i] = compute_energy_fft_gpu(test_seq)\n",
    "        return energies\n",
    "\n",
    "\n",
    "def tabu_search_local_gpu(\n",
    "    sequence: np.ndarray, \n",
    "    config: TabuSearchConfig, \n",
    "    verbose: bool = False\n",
    ") -> Tuple[np.ndarray, float, int]:\n",
    "    \"\"\"GPU-optimized tabu search.\"\"\"\n",
    "    n = len(sequence)\n",
    "    current = sequence.copy()\n",
    "    current_energy = compute_energy_fft_gpu(current)\n",
    "    \n",
    "    best = current.copy()\n",
    "    best_energy = current_energy\n",
    "    \n",
    "    tabu_list = TabuList(config.tabu_tenure)\n",
    "    \n",
    "    iterations_without_improvement = 0\n",
    "    total_iterations = 0\n",
    "    \n",
    "    while iterations_without_improvement < config.max_iterations:\n",
    "        total_iterations += 1\n",
    "        \n",
    "        # GPU: Evaluate ALL flips in parallel\n",
    "        flip_energies = evaluate_all_flips_gpu(current)\n",
    "        \n",
    "        # Find best moves\n",
    "        best_move = int(np.argmin(flip_energies))\n",
    "        best_move_energy = flip_energies[best_move]\n",
    "        \n",
    "        # Find best non-tabu move\n",
    "        best_non_tabu_move = -1\n",
    "        best_non_tabu_energy = float('inf')\n",
    "        \n",
    "        sorted_indices = np.argsort(flip_energies)\n",
    "        for idx in sorted_indices:\n",
    "            if not tabu_list.is_tabu(idx):\n",
    "                best_non_tabu_move = idx\n",
    "                best_non_tabu_energy = flip_energies[idx]\n",
    "                break\n",
    "        \n",
    "        # Aspiration criterion\n",
    "        if best_move_energy < best_energy - config.aspiration_threshold:\n",
    "            chosen_move = best_move\n",
    "            chosen_energy = best_move_energy\n",
    "        elif best_non_tabu_move >= 0:\n",
    "            chosen_move = best_non_tabu_move\n",
    "            chosen_energy = best_non_tabu_energy\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        # Apply move\n",
    "        current[chosen_move] *= -1\n",
    "        current_energy = chosen_energy\n",
    "        tabu_list.add(chosen_move)\n",
    "        \n",
    "        # Update best\n",
    "        if current_energy < best_energy:\n",
    "            best = current.copy()\n",
    "            best_energy = current_energy\n",
    "            iterations_without_improvement = 0\n",
    "        else:\n",
    "            iterations_without_improvement += 1\n",
    "    \n",
    "    return best, best_energy, total_iterations\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# GPU-OPTIMIZED MEMETIC TABU SEARCH\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class MTSConfig:\n",
    "    population_size: int = 50\n",
    "    elite_size: int = 5\n",
    "    tabu_tenure: int = 7\n",
    "    local_search_iterations: int = 100\n",
    "    crossover_rate: float = 0.8\n",
    "    mutation_rate: float = 0.1\n",
    "    max_generations: int = 100\n",
    "    stagnation_limit: int = 20\n",
    "    intensify_threshold: int = 5\n",
    "    diversify_threshold: int = 10\n",
    "    # GPU optimization: number of individuals to apply local search to\n",
    "    parallel_local_search: int = 5\n",
    "\n",
    "\n",
    "class MemeticTabuSearchGPU:\n",
    "    \"\"\"GPU-optimized Memetic Tabu Search.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_qubits: int, config: MTSConfig = None):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.config = config or MTSConfig()\n",
    "        \n",
    "        self.best_sequence = None\n",
    "        self.best_energy = float('inf')\n",
    "        self.best_merit_factor = 0.0\n",
    "        self.history = []\n",
    "        \n",
    "        self.total_evaluations = 0\n",
    "        self.generation = 0\n",
    "        \n",
    "        # Timing statistics\n",
    "        self.timing = {\n",
    "            'fitness_eval': 0.0,\n",
    "            'local_search': 0.0,\n",
    "            'genetic_ops': 0.0\n",
    "        }\n",
    "    \n",
    "    def evaluate_population(self, population: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"GPU OPTIMIZATION: Batch fitness evaluation.\"\"\"\n",
    "        start = time.time()\n",
    "        fitness = compute_energy_batch_gpu(population)\n",
    "        self.timing['fitness_eval'] += time.time() - start\n",
    "        self.total_evaluations += len(population)\n",
    "        return fitness\n",
    "    \n",
    "    def select_parents_gpu(self, population: np.ndarray, fitness: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Vectorized tournament selection.\"\"\"\n",
    "        tournament_size = 3\n",
    "        n_parents = len(population)\n",
    "        \n",
    "        # Generate all tournament indices at once\n",
    "        all_tournaments = np.random.randint(0, len(population), \n",
    "                                            size=(n_parents, tournament_size))\n",
    "        \n",
    "        # Find winners\n",
    "        tournament_fitness = fitness[all_tournaments]\n",
    "        winner_local_idx = np.argmin(tournament_fitness, axis=1)\n",
    "        winner_idx = all_tournaments[np.arange(n_parents), winner_local_idx]\n",
    "        \n",
    "        return population[winner_idx].copy()\n",
    "    \n",
    "    def crossover_batch_gpu(self, parents: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"GPU OPTIMIZATION: Vectorized batch crossover.\"\"\"\n",
    "        n_pairs = len(parents) // 2\n",
    "        n = self.n_qubits\n",
    "        \n",
    "        if GPU_AVAILABLE and n_pairs > 10:\n",
    "            parents_gpu = cp.asarray(parents)\n",
    "            offspring = cp.zeros_like(parents_gpu)\n",
    "            \n",
    "            # Generate crossover decisions\n",
    "            do_crossover = cp.random.random(n_pairs) < self.config.crossover_rate\n",
    "            \n",
    "            # Generate crossover points\n",
    "            points = cp.sort(cp.random.randint(0, n, size=(n_pairs, 2)), axis=1)\n",
    "            \n",
    "            for i in range(n_pairs):\n",
    "                p1, p2 = parents_gpu[2*i], parents_gpu[2*i + 1]\n",
    "                \n",
    "                if do_crossover[i]:\n",
    "                    pt1, pt2 = int(points[i, 0]), int(points[i, 1])\n",
    "                    c1 = cp.concatenate([p1[:pt1], p2[pt1:pt2], p1[pt2:]])\n",
    "                    c2 = cp.concatenate([p2[:pt1], p1[pt1:pt2], p2[pt2:]])\n",
    "                else:\n",
    "                    c1, c2 = p1.copy(), p2.copy()\n",
    "                \n",
    "                offspring[2*i] = c1\n",
    "                offspring[2*i + 1] = c2\n",
    "            \n",
    "            return offspring.get()\n",
    "        else:\n",
    "            # CPU fallback\n",
    "            offspring = []\n",
    "            for i in range(0, len(parents) - 1, 2):\n",
    "                c1, c2 = self._crossover_pair(parents[i], parents[i+1])\n",
    "                offspring.extend([c1, c2])\n",
    "            if len(parents) % 2 == 1:\n",
    "                offspring.append(parents[-1].copy())\n",
    "            return np.array(offspring[:self.config.population_size])\n",
    "    \n",
    "    def _crossover_pair(self, p1: np.ndarray, p2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if np.random.random() > self.config.crossover_rate:\n",
    "            return p1.copy(), p2.copy()\n",
    "        \n",
    "        n = len(p1)\n",
    "        points = sorted(np.random.choice(n, size=2, replace=False))\n",
    "        \n",
    "        c1, c2 = p1.copy(), p2.copy()\n",
    "        c1[points[0]:points[1]] = p2[points[0]:points[1]]\n",
    "        c2[points[0]:points[1]] = p1[points[0]:points[1]]\n",
    "        \n",
    "        return c1, c2\n",
    "    \n",
    "    def mutate_batch_gpu(self, population: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"GPU OPTIMIZATION: Vectorized batch mutation.\"\"\"\n",
    "        if GPU_AVAILABLE:\n",
    "            pop_gpu = cp.asarray(population)\n",
    "            # Generate mutation mask for entire population at once\n",
    "            mutation_mask = cp.random.random(pop_gpu.shape) < self.config.mutation_rate\n",
    "            # Apply mutations: flip where mask is True\n",
    "            pop_gpu = cp.where(mutation_mask, -pop_gpu, pop_gpu)\n",
    "            return pop_gpu.get()\n",
    "        else:\n",
    "            mutated = population.copy()\n",
    "            mask = np.random.random(mutated.shape) < self.config.mutation_rate\n",
    "            mutated[mask] *= -1\n",
    "            return mutated\n",
    "    \n",
    "    def local_search_parallel(self, population: np.ndarray, fitness: np.ndarray, \n",
    "                              n_apply: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Apply local search to top n individuals.\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # Select top individuals for local search\n",
    "        top_indices = np.argsort(fitness)[:n_apply]\n",
    "        \n",
    "        config = TabuSearchConfig(\n",
    "            tabu_tenure=self.config.tabu_tenure,\n",
    "            max_iterations=self.config.local_search_iterations\n",
    "        )\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            improved, energy, _ = tabu_search_local_gpu(population[idx], config)\n",
    "            population[idx] = improved\n",
    "            fitness[idx] = energy\n",
    "        \n",
    "        self.timing['local_search'] += time.time() - start\n",
    "        return population, fitness\n",
    "    \n",
    "    def intensification(self, population: np.ndarray, fitness: np.ndarray) -> np.ndarray:\n",
    "        elite_indices = np.argsort(fitness)[:self.config.elite_size]\n",
    "        elite = population[elite_indices]\n",
    "        \n",
    "        new_population = list(population)\n",
    "        for elite_seq in elite:\n",
    "            for _ in range(2):\n",
    "                variant = elite_seq.copy()\n",
    "                n_flips = np.random.randint(1, 3)\n",
    "                flip_positions = np.random.choice(len(variant), size=n_flips, replace=False)\n",
    "                for pos in flip_positions:\n",
    "                    variant[pos] *= -1\n",
    "                new_population.append(variant)\n",
    "        \n",
    "        new_population = np.array(new_population)\n",
    "        new_fitness = self.evaluate_population(new_population)\n",
    "        best_indices = np.argsort(new_fitness)[:self.config.population_size]\n",
    "        \n",
    "        return new_population[best_indices]\n",
    "    \n",
    "    def diversification(self, population: np.ndarray, fitness: np.ndarray) -> np.ndarray:\n",
    "        elite_indices = np.argsort(fitness)[:self.config.elite_size]\n",
    "        elite = population[elite_indices]\n",
    "        \n",
    "        n_new = self.config.population_size - self.config.elite_size\n",
    "        new_solutions = np.random.choice([-1, 1], size=(n_new, self.n_qubits))\n",
    "        \n",
    "        return np.vstack([elite, new_solutions])\n",
    "    \n",
    "    def run(self, quantum_seeds: np.ndarray, verbose: bool = True) -> Tuple[np.ndarray, float, float]:\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"GPU Acceleration: {'ENABLED' if GPU_AVAILABLE else 'DISABLED'}\")\n",
    "            print(f\"Sequence length: {self.n_qubits}\")\n",
    "            print(f\"Population size: {self.config.population_size}\")\n",
    "        \n",
    "        population = quantum_seeds.copy()\n",
    "        fitness = self.evaluate_population(population)\n",
    "        \n",
    "        best_idx = np.argmin(fitness)\n",
    "        self.best_sequence = population[best_idx].copy()\n",
    "        self.best_energy = fitness[best_idx]\n",
    "        self.best_merit_factor = compute_merit_factor_fft(self.best_sequence)\n",
    "        \n",
    "        stagnation_counter = 0\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nInitial best: E={self.best_energy:.2f}, MF={self.best_merit_factor:.4f}\")\n",
    "            print(\"\\n[Generation Progress]\")\n",
    "        \n",
    "        for gen in range(self.config.max_generations):\n",
    "            self.generation = gen\n",
    "            gen_start = time.time()\n",
    "            \n",
    "            # Selection\n",
    "            parents = self.select_parents_gpu(population, fitness)\n",
    "            \n",
    "            # Crossover and Mutation (GPU-optimized)\n",
    "            start = time.time()\n",
    "            offspring = self.crossover_batch_gpu(parents)\n",
    "            offspring = self.mutate_batch_gpu(offspring)\n",
    "            self.timing['genetic_ops'] += time.time() - start\n",
    "            \n",
    "            # Evaluate offspring\n",
    "            offspring_fitness = self.evaluate_population(offspring)\n",
    "            \n",
    "            # Parallel local search on top individuals\n",
    "            offspring, offspring_fitness = self.local_search_parallel(\n",
    "                offspring, offspring_fitness, \n",
    "                n_apply=self.config.parallel_local_search\n",
    "            )\n",
    "            \n",
    "            # Elitism\n",
    "            elite_indices = np.argsort(fitness)[:self.config.elite_size]\n",
    "            worst_offspring_indices = np.argsort(offspring_fitness)[-self.config.elite_size:]\n",
    "            \n",
    "            for i, elite_idx in enumerate(elite_indices):\n",
    "                offspring[worst_offspring_indices[i]] = population[elite_idx].copy()\n",
    "                offspring_fitness[worst_offspring_indices[i]] = fitness[elite_idx]\n",
    "            \n",
    "            population = offspring\n",
    "            fitness = offspring_fitness\n",
    "            \n",
    "            # Update best\n",
    "            gen_best_idx = np.argmin(fitness)\n",
    "            gen_best_energy = fitness[gen_best_idx]\n",
    "            \n",
    "            improved = False\n",
    "            if gen_best_energy < self.best_energy:\n",
    "                self.best_sequence = population[gen_best_idx].copy()\n",
    "                self.best_energy = gen_best_energy\n",
    "                self.best_merit_factor = compute_merit_factor_fft(self.best_sequence)\n",
    "                stagnation_counter = 0\n",
    "                improved = True\n",
    "            else:\n",
    "                stagnation_counter += 1\n",
    "            \n",
    "            gen_time = time.time() - gen_start\n",
    "            \n",
    "            self.history.append({\n",
    "                'generation': gen,\n",
    "                'best_energy': self.best_energy,\n",
    "                'merit_factor': self.best_merit_factor,\n",
    "                'gen_best_energy': gen_best_energy,\n",
    "                'gen_mean_energy': np.mean(fitness),\n",
    "                'gen_std_energy': np.std(fitness),\n",
    "                'gen_time': gen_time,\n",
    "                'improved': improved\n",
    "            })\n",
    "            \n",
    "            if verbose and (gen % 10 == 0 or improved):\n",
    "                print(f\"  Gen {gen:3d}: Best E={self.best_energy:.2f}, \"\n",
    "                      f\"MF={self.best_merit_factor:.4f}, Time={gen_time:.3f}s\"\n",
    "                      + (\" *\" if improved else \"\"))\n",
    "            \n",
    "            # Intensification/Diversification\n",
    "            if stagnation_counter == self.config.intensify_threshold:\n",
    "                if verbose:\n",
    "                    print(f\"  Gen {gen}: Applying intensification...\")\n",
    "                population = self.intensification(population, fitness)\n",
    "                fitness = self.evaluate_population(population)\n",
    "            \n",
    "            elif stagnation_counter == self.config.diversify_threshold:\n",
    "                if verbose:\n",
    "                    print(f\"  Gen {gen}: Applying diversification...\")\n",
    "                population = self.diversification(population, fitness)\n",
    "                fitness = self.evaluate_population(population)\n",
    "            \n",
    "            if stagnation_counter >= self.config.stagnation_limit:\n",
    "                if verbose:\n",
    "                    print(f\"\\n  Stopping: No improvement for {stagnation_counter} generations\")\n",
    "                break\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"-\" * 70)\n",
    "            print(f\"Timing Breakdown:\")\n",
    "            print(f\"  Fitness evaluation: {self.timing['fitness_eval']:.2f}s\")\n",
    "            print(f\"  Local search: {self.timing['local_search']:.2f}s\")\n",
    "            print(f\"  Genetic operations: {self.timing['genetic_ops']:.2f}s\")\n",
    "        \n",
    "        return self.best_sequence, self.best_energy, self.best_merit_factor\n",
    "        \n",
    " \n",
    "def run_runtime_benchmark(qubit_range: List[int], n_layers: int = 5, \n",
    "                          n_pop: int = 50, max_gen: int = 20) -> dict:\n",
    "    \"\"\"\n",
    "    Benchmark runtime scaling across different qubit counts.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'qubits': qubit_range,\n",
    "        'vqe_times': [],\n",
    "        'mts_times': [],\n",
    "        'total_times': [],\n",
    "        'best_energies': [],\n",
    "        'merit_factors': []\n",
    "    }\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"RUNTIME SCALING BENCHMARK\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for n_qubits in qubit_range:\n",
    "        print(f\"\\n--- Benchmarking {n_qubits} qubits ---\")\n",
    "        \n",
    "        # VQE phase\n",
    "        vqe_start = time.time()\n",
    "        vqe_history, quantum_pop = generate_quantum_pop(n_qubits, n_layers, n_pop, max_iter=100)\n",
    "        vqe_time = time.time() - vqe_start\n",
    "        \n",
    "        # MTS phase\n",
    "        config = MTSConfig(\n",
    "            population_size=n_pop,\n",
    "            max_generations=max_gen,\n",
    "            local_search_iterations=50,\n",
    "            tabu_tenure=max(5, n_qubits // 2)\n",
    "        )\n",
    "        \n",
    "        mts = MemeticTabuSearchGPU(n_qubits, config)\n",
    "        mts_start = time.time()\n",
    "        best_seq, best_energy, best_mf = mts.run(quantum_pop, verbose=False)\n",
    "        mts_time = time.time() - mts_start\n",
    "        \n",
    "        results['vqe_times'].append(vqe_time)\n",
    "        results['mts_times'].append(mts_time)\n",
    "        results['total_times'].append(vqe_time + mts_time)\n",
    "        results['best_energies'].append(best_energy)\n",
    "        results['merit_factors'].append(best_mf)\n",
    "        \n",
    "        print(f\"  VQE time: {vqe_time:.2f}s, MTS time: {mts_time:.2f}s\")\n",
    "        print(f\"  Best energy: {best_energy:.2f}, Merit factor: {best_mf:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_complete_hybrid_workflow_gpu(\n",
    "    quantum_population: np.ndarray,\n",
    "    n_qubits: int,\n",
    "    mts_config: Optional[MTSConfig] = None,\n",
    "    verbose: bool = True\n",
    ") -> Tuple[np.ndarray, float, float, dict]:\n",
    "    \"\"\"GPU-optimized hybrid workflow.\"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"GPU-ACCELERATED QUANTUM-CLASSICAL HYBRID WORKFLOW\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"\\nGPU Status: {'ENABLED (CuPy)' if GPU_AVAILABLE else 'DISABLED (NumPy fallback)'}\")\n",
    "        print(f\"Quantum input: {len(quantum_population)} sequences of length {n_qubits}\")\n",
    "    \n",
    "    if mts_config is None:\n",
    "        mts_config = MTSConfig(\n",
    "            population_size=max(100, len(quantum_population)),\n",
    "            max_generations=100,\n",
    "            local_search_iterations=100,\n",
    "            tabu_tenure=max(5, n_qubits // 2),\n",
    "            stagnation_limit=30,\n",
    "            parallel_local_search=5\n",
    "        )\n",
    "    \n",
    "    mts = MemeticTabuSearchGPU(n_qubits, mts_config)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    best_seq, best_energy, best_mf = mts.run(\n",
    "        quantum_seeds=quantum_population,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    statistics = {\n",
    "        'total_evaluations': mts.total_evaluations,\n",
    "        'generations': mts.generation + 1,\n",
    "        'elapsed_time': elapsed_time,\n",
    "        'history': mts.history,\n",
    "        'quantum_seed_size': len(quantum_population),\n",
    "        'timing_breakdown': mts.timing\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nWorkflow Statistics:\")\n",
    "        print(f\"  Total evaluations: {statistics['total_evaluations']}\")\n",
    "        print(f\"  Generations: {statistics['generations']}\")\n",
    "        print(f\"  Total time: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return best_seq, best_energy, best_mf, statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb3761a-2cfe-46d3-bdbe-281cffe39102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    n_qubits = 7\n",
    "    n_layers = 5\n",
    "    n_pop = 100\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"LABS OPTIMIZATION - GPU-ACCELERATED HYBRID QUANTUM-CLASSICAL\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Generate quantum population\n",
    "    vqe_history, quantum_population = generate_quantum_pop(n_qubits, n_layers, n_pop)\n",
    "    print(f\"Quantum population shape: {np.shape(quantum_population)}\")\n",
    "    \n",
    "    # Configure MTS\n",
    "    config = MTSConfig(\n",
    "        population_size=n_pop,\n",
    "        max_generations=50,\n",
    "        local_search_iterations=75,\n",
    "        tabu_tenure=6,\n",
    "        parallel_local_search=5\n",
    "    )\n",
    "    \n",
    "    # Run hybrid workflow\n",
    "    best_seq, best_energy, best_mf, stats = run_complete_hybrid_workflow_gpu(\n",
    "        quantum_population=quantum_population,\n",
    "        n_qubits=n_qubits,\n",
    "        mts_config=config,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"FINAL RESULT\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Best sequence: {best_seq.tolist()}\")\n",
    "    print(f\"Energy: {best_energy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [cuda-q-v0.13.0]",
   "language": "python",
   "name": "python3_n83d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
